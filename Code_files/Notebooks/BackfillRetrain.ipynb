{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {},
      "source": [
        "!pip install yfinance --upgrade --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "collapsed": false
      },
      "source": [
        "# Step 1: Load actual stock volume from yfinance\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "today = datetime.today().date()\n",
        "#today = datetime.strptime(\"2025-04-30\", \"%Y-%m-%d\").date()\n",
        "\n",
        "#Pull only one day's worth of data\n",
        "df = yf.download(\"NVDA\", start=str(today), end=str(today + timedelta(days=1)))\n",
        "\n",
        "#Reset index for cleaner formatting\n",
        "df.reset_index(inplace=True)\n",
        "\n",
        "# Check if DataFrame is empty\n",
        "if df.empty:\n",
        "    display(\"No stock data returned from yfinance. Stopping execution.\")\n",
        "    \n",
        "    # If you're running in a Synapse notebook, exit cleanly\n",
        "    from notebookutils import mssparkutils\n",
        "    mssparkutils.notebook.exit(\"No data found - stopping notebook\")\n",
        "else:\n",
        "    display(\"Stock data successfully retrieved, proceeding with analysis.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "collapsed": false
      },
      "source": [
        "# Step 2: Load the Stream Data from blob\n",
        "from py4j.java_gateway import java_import\n",
        "from datetime import datetime\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "# Step 1: Define today\n",
        "today = datetime.today().date()\n",
        "\n",
        "# Step 2: Set up Hadoop FS for listing files\n",
        "stream_path = \"abfss://team12blobcontainer@team12storage.dfs.core.windows.net/Medallion/Gold/Stream/\"\n",
        "java_import(spark._jvm, 'org.apache.hadoop.fs.Path')\n",
        "fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
        "path = spark._jvm.Path(stream_path)\n",
        "files = fs.listStatus(path)\n",
        "\n",
        "# Step 3: Extract valid .json file names with dates < today\n",
        "json_files = [f.getPath().getName() for f in files if f.isFile() and f.getPath().getName().endswith(\".json\")]\n",
        "\n",
        "dated_files = []\n",
        "for fname in json_files:\n",
        "    try:\n",
        "        file_date = datetime.strptime(fname.replace(\".json\", \"\"), \"%Y-%m-%d\").date()\n",
        "        if file_date < today:\n",
        "            dated_files.append(file_date)\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "# Step 4: Get most recent available file and load it\n",
        "if dated_files:\n",
        "    nearest_date = max(dated_files)\n",
        "    file_path = f\"{stream_path}{nearest_date}.json\"\n",
        "    print(f\"Loaded file: {file_path}\")\n",
        "\n",
        "    streaming_df = spark.read.option(\"multiline\", \"true\").json(file_path)\n",
        "    #display(streaming_df.limit(10))\n",
        "else:\n",
        "    print(\"No previous date file found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "collapsed": false
      },
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.ml import PipelineModel\n",
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "# Step 1: Load previous predictions and input snapshots\n",
        "container = \"team12blobcontainer\"\n",
        "storage_account_name = \"team12storage\"\n",
        "abfss_base_path = f\"abfss://{container}@{storage_account_name}.dfs.core.windows.net/\"\n",
        "prediction_date = streaming_df.select(\"tweet_created_at_date\").first()[\"tweet_created_at_date\"]\n",
        "predicted_df = spark.read.parquet(f\"{abfss_base_path}Prediction/Streaming/{prediction_date}/\")\n",
        "display(predicted_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql.functions import col, to_date\n",
        "# Step 2: Update the batch prediction file\n",
        "aligned_predicted_df = predicted_df.select(\n",
        "    to_date(\"tweet_created_at_date\").alias(\"tweet_created_at_date\"),\n",
        "    col(\"predicted_next_day_volume\").cast(\"double\")\n",
        ")\n",
        "update_batch_prediction_path = f\"{abfss_base_path}Prediction/Batch/data\"\n",
        "aligned_predicted_df.write.mode(\"append\").parquet(update_batch_prediction_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Read the gold layer file\n",
        "origin_gold_df = spark.read.parquet(f\"{abfss_base_path}Medallion/Gold/dataset_updated/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "from pyspark.sql.functions import col, to_date\n",
        "# Step 3: Add df[\"Volume\"] to streaming_inputs_df as new column and append back to gold layer\n",
        "from pyspark.sql.functions import lit\n",
        "\n",
        "# 1. Extract the volume value from the yfinance DataFrame (assume one row)\n",
        "actual_volume = int(df[\"Volume\"].iloc[0])\n",
        "\n",
        "# 2. Add it as a new column to your Spark streaming input DataFrame\n",
        "streaming_df = streaming_df.withColumn(\"next_available_volume\", lit(actual_volume))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "source": [
        "# 3: Cast and select columns in the same order as gold schema\n",
        "aligned_streaming_df = streaming_df.select(\n",
        "    col(\"user_id\").cast(\"string\"),\n",
        "    to_date(\"tweet_created_at_date\").alias(\"tweet_created_at_date\"),   # from string to date\n",
        "    col(\"is_blue_verified\").cast(\"boolean\"),\n",
        "    to_date(\"account_created_at\").alias(\"account_created_at\"),         # from string to date\n",
        "    col(\"followers_count\").cast(\"int\"),\n",
        "    col(\"friends_count\").cast(\"int\"),\n",
        "    col(\"account_favourites_count\").cast(\"int\"),\n",
        "    col(\"listed_count\").cast(\"int\"),\n",
        "    col(\"media_count\").cast(\"int\"),\n",
        "    col(\"account_possibly_sensitive\").cast(\"boolean\"),\n",
        "    col(\"rest_id\").cast(\"string\"),\n",
        "    col(\"tweet_created_at_time\").cast(\"string\"),\n",
        "    col(\"view_count\").cast(\"string\"),\n",
        "    col(\"retweet_count\").cast(\"int\"),\n",
        "    col(\"reply_count\").cast(\"int\"),\n",
        "    col(\"quote_count\").cast(\"int\"),\n",
        "    col(\"favorite_count\").cast(\"int\"),\n",
        "    col(\"tweet_possibly_sensitive\").cast(\"boolean\"),\n",
        "    col(\"full_text\").cast(\"string\"),\n",
        "    col(\"sentiment_score\").cast(\"double\"),\n",
        "    col(\"interaction_score\").cast(\"double\"),\n",
        "    col(\"favorite_ratio\").cast(\"double\"),\n",
        "    col(\"reply_ratio\").cast(\"double\"),\n",
        "    col(\"account_age_days\").cast(\"int\"),\n",
        "    col(\"credibility_score\").cast(\"double\"),\n",
        "    col(\"follower_activity_score\").cast(\"double\"),\n",
        "    col(\"day_of_week\").cast(\"string\"),\n",
        "    col(\"is_viral\").cast(\"boolean\"),\n",
        "    col(\"is_new_account\").cast(\"boolean\"),\n",
        "    col(\"is_influencer\").cast(\"boolean\"),\n",
        "    col(\"current_volume\").cast(\"int\"),\n",
        "    col(\"current_open\").cast(\"double\"),\n",
        "    col(\"current_close\").cast(\"double\"),\n",
        "    col(\"current_high\").cast(\"double\"),\n",
        "    col(\"current_low\").cast(\"double\"),\n",
        "    col(\"next_available_volume\").cast(\"int\")\n",
        ")\n",
        "\n",
        "# 4. Save the enriched streaming data to your Gold layer (append mode)\n",
        "gold_output_path = f\"{abfss_base_path}Medallion/Gold/dataset_updated\"\n",
        "\n",
        "aligned_streaming_df.write.mode(\"append\").parquet(gold_output_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Step 4: Define retrain function (if should_retrain = True)\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
        "from pyspark.ml import Pipeline, PipelineModel\n",
        "\n",
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "import datetime\n",
        "from py4j.java_gateway import java_import\n",
        "\n",
        "def retrain():\n",
        "\n",
        "    # Load Updated Gold Dataset\n",
        "    updated_gold_df = spark.read.parquet(gold_output_path)\n",
        "\n",
        "    # Load the pipeline\n",
        "    pipeline_path = \"abfss://team12blobcontainer@team12storage.dfs.core.windows.net/Model/preprocess_pipeline\"\n",
        "    preprocessing_pipeline = PipelineModel.load(pipeline_path)\n",
        "\n",
        "    # Step 1: Preprocess the data (same as initial pipeline setup)\n",
        "    # Step 1.1: Convert Boolean to Integer\n",
        "    updated_gold_df = updated_gold_df.withColumn(\"is_viral_int\", col(\"is_viral\").cast(\"int\")) \\\n",
        "        .withColumn(\"is_new_account_int\", col(\"is_new_account\").cast(\"int\")) \\\n",
        "        .withColumn(\"is_blue_verified_int\", col(\"is_blue_verified\").cast(\"int\")) \\\n",
        "        .withColumn(\"is_influencer_int\", col(\"is_influencer\").cast(\"int\"))\n",
        "\n",
        "    # Step 1.2: Define required columns\n",
        "    required_cols = [\n",
        "        \"sentiment_score\", \"interaction_score\", \"favorite_ratio\", \"reply_ratio\",\n",
        "        \"followers_count\", \"friends_count\", \"account_favourites_count\",\n",
        "        \"listed_count\", \"media_count\", \"account_age_days\", \"credibility_score\",\n",
        "        \"follower_activity_score\", \"current_open\", \"current_close\",\n",
        "        \"current_high\", \"current_low\", \"current_volume\", \"next_available_volume\",\n",
        "        \"day_of_week\", \"is_viral_int\", \"is_new_account_int\", \"is_blue_verified_int\", \"is_influencer_int\"\n",
        "    ]\n",
        "\n",
        "    # Step 1.3: Drop nulls\n",
        "    updated_gold_df = updated_gold_df.dropna(subset=required_cols)\n",
        "\n",
        "    # Step 1.4: Reuse the same preprocessing pipeline\n",
        "    processed_df = preprocessing_pipeline.transform(updated_gold_df)\n",
        "    print(\"Preprocessing Complete.\")\n",
        "\n",
        "    # Step 2.1: Split the data\n",
        "    train_df, test_df = processed_df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "    # Step 2.2: Define the model\n",
        "    rf = RandomForestRegressor(\n",
        "        featuresCol=\"scaled_features\", \n",
        "        labelCol=\"next_available_volume\", \n",
        "        predictionCol=\"prediction\",\n",
        "        seed=42\n",
        "    )\n",
        "\n",
        "    # Step 2.3: Build the parameter grid\n",
        "    param_grid = ParamGridBuilder() \\\n",
        "        .addGrid(rf.numTrees, [20, 50, 100]) \\\n",
        "        .addGrid(rf.maxDepth, [5, 10, 15]) \\\n",
        "        .addGrid(rf.minInstancesPerNode, [1, 5]) \\\n",
        "        .build()\n",
        "\n",
        "    # Step 2.4: Define evaluator\n",
        "    evaluator = RegressionEvaluator(\n",
        "        labelCol=\"next_available_volume\", \n",
        "        predictionCol=\"prediction\", \n",
        "        metricName=\"rmse\"\n",
        "    )\n",
        "\n",
        "    # Step 2.5: CrossValidator setup\n",
        "    cv = CrossValidator(\n",
        "        estimator=rf,\n",
        "        estimatorParamMaps=param_grid,\n",
        "        evaluator=evaluator,\n",
        "        numFolds=3,\n",
        "        parallelism=2\n",
        "    )\n",
        "\n",
        "    # Step 2.6: Fit the model on training data\n",
        "    cv_model = cv.fit(train_df)\n",
        "\n",
        "    # Step 2.7: Evaluate on test data\n",
        "    new_best_model = cv_model.bestModel\n",
        "    predictions = new_best_model.transform(test_df)\n",
        "\n",
        "    rmse = evaluator.evaluate(predictions)\n",
        "    r2 = RegressionEvaluator(\n",
        "        labelCol=\"next_available_volume\", \n",
        "        predictionCol=\"prediction\", \n",
        "        metricName=\"r2\"\n",
        "    ).evaluate(predictions)\n",
        "\n",
        "    print(f\"Best model params:\")\n",
        "    print(f\" - numTrees: {new_best_model.getNumTrees}\")\n",
        "    print(f\" - maxDepth: {new_best_model.getOrDefault('maxDepth')}\")\n",
        "    print(f\" - minInstancesPerNode: {new_best_model.getOrDefault('minInstancesPerNode')}\")\n",
        "    print(f\"New RMSE: {rmse}\")\n",
        "    print(f\"New R²: {r2}\")\n",
        "\n",
        "    # Step 3: Save best trained model\n",
        "    # Step 3.1 Archive the older version\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    abfss_base_path = f\"abfss://{container}@{storage_account_name}.dfs.core.windows.net/Model\"\n",
        "    current_model_path = f\"{abfss_base_path}/stock_volume_rf_model\"\n",
        "    archived_model_path = f\"{abfss_base_path}/archive/stock_volume_rf_model_{timestamp}\"\n",
        "\n",
        "    # Use Hadoop API to rename (move) the directory\n",
        "    java_import(spark._jvm, 'org.apache.hadoop.fs.Path')\n",
        "    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
        "\n",
        "    src_path = spark._jvm.Path(current_model_path)\n",
        "    dst_path = spark._jvm.Path(archived_model_path)\n",
        "\n",
        "    try:\n",
        "        if fs.exists(src_path):\n",
        "            fs.rename(src_path, dst_path)\n",
        "            print(f\"Archived old model to: {archived_model_path}\")\n",
        "        else:\n",
        "            print(\"No existing model to archive.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to archive old model: {e}\")\n",
        "\n",
        "    # Step 3.2: Save new model\n",
        "    new_best_model.write().overwrite().save(current_model_path)\n",
        "    print(f\"New model saved to: {current_model_path}\")\n",
        "\n",
        "    return new_best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "source": [
        "# Step 5: Compare the actual stock volume with the prediction\n",
        "predicted_volume = predicted_df.select(\"predicted_next_day_volume\").first()[\"predicted_next_day_volume\"]\n",
        "\n",
        "# Compute relative error\n",
        "error = abs(actual_volume - predicted_volume) / actual_volume\n",
        "\n",
        "#Define the threshold to retrain\n",
        "RETRAIN_THRESHOLD = 0.10\n",
        "\n",
        "if error > RETRAIN_THRESHOLD:\n",
        "    print(\"Significant prediction error — retraining required.\")\n",
        "    should_retrain = True\n",
        "    trained_model = retrain()\n",
        "else:\n",
        "    print(\"Prediction within acceptable range — no retraining.\")\n",
        "    should_retrain = False"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": true,
    "language_info": {
      "name": "python"
    }
  }
}