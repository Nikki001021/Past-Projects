{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "python"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {
        "db8d025b-44c4-4bd5-8649-14232a82a748": {
          "type": "Synapse.DataFrame",
          "sync_state": {
            "table": {
              "rows": [
                {
                  "0": "2025-04-28",
                  "1": "2.8666761305341184E8"
                }
              ],
              "schema": [
                {
                  "key": "0",
                  "name": "tweet_created_at_date",
                  "type": "string"
                },
                {
                  "key": "1",
                  "name": "predicted_next_day_volume",
                  "type": "double"
                }
              ],
              "truncated": false
            },
            "isSummary": false,
            "language": "scala",
            "wranglerEntryContext": {
              "dataframeType": "pyspark"
            }
          },
          "persist_state": {
            "view": {
              "type": "details",
              "tableOptions": {},
              "chartOptions": {
                "chartType": "bar",
                "aggregationType": "sum",
                "categoryFieldKeys": [
                  "0"
                ],
                "seriesFieldKeys": [
                  "1"
                ],
                "isStacked": false
              }
            }
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "stockPredict",
              "statement_id": 2,
              "statement_ids": [
                2
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "39",
              "normalized_state": "finished",
              "queued_time": "2025-05-05T22:01:48.1060917Z",
              "session_start_time": "2025-05-05T22:01:48.106973Z",
              "execution_start_time": "2025-05-05T22:03:45.0895809Z",
              "execution_finish_time": "2025-05-05T22:03:45.3265722Z",
              "parent_msg_id": "4bceb7db-751b-475a-ba15-553488357eae"
            },
            "text/plain": "StatementMeta(stockPredict, 39, 2, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "# Step 1: Set up Blob Storage Config\n",
        "spark.conf.set(\n",
        "  \"fs.azure.account.key.team12storage.blob.core.windows.net\",\n",
        "  \"ZdpCqxRwuQbJ8fUYN7EMbTLjOyj+I9rI6IjcQ/uU+4aFG46TQqjiz1Piq9SWKGkkjszo3r94l26M+AStR5PvPg==\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "stockPredict",
              "statement_id": 3,
              "statement_ids": [
                3
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "39",
              "normalized_state": "finished",
              "queued_time": "2025-05-05T22:04:49.8658938Z",
              "session_start_time": null,
              "execution_start_time": "2025-05-05T22:04:49.8669986Z",
              "execution_finish_time": "2025-05-05T22:04:58.9042473Z",
              "parent_msg_id": "0623ee68-43f3-4b3b-9c5a-4ab336015157"
            },
            "text/plain": "StatementMeta(stockPredict, 39, 3, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully read streaming data from: wasbs://team12blobcontainer@team12storage.blob.core.windows.net/Medallion/Gold/Stream/2025-04-28.json\n"
          ]
        }
      ],
      "metadata": {},
      "source": [
        "# Step 2: Read Stream Data From Blob Storage\n",
        "from pyspark.sql.functions import lit\n",
        "from datetime import datetime, timedelta\n",
        "from notebookutils import mssparkutils\n",
        "\n",
        "start_date = (datetime.now()-timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
        "#start_date = \"2025-04-28\"\n",
        "input_path = f\"wasbs://team12blobcontainer@team12storage.blob.core.windows.net/Medallion/Gold/Stream/{start_date}.json\"\n",
        "\n",
        "try:\n",
        "    # First verify file exists\n",
        "    if mssparkutils.fs.exists(input_path):\n",
        "        streaming_df = spark.read.option(\"multiline\", \"true\").json(input_path)\n",
        "        print(f\"Successfully read streaming data from: {input_path}\")\n",
        "        \n",
        "        # Add processing timestamp column\n",
        "        streaming_df = streaming_df.withColumn(\"processing_time\", lit(datetime.now()))\n",
        "    else:\n",
        "        print(f\"File not found: {input_path}\")\n",
        "        # End notebook execution if file doesn't exist\n",
        "        mssparkutils.notebook.exit(\"Input file not found - stopping execution\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"Error reading stream data: {str(e)}\")\n",
        "    # End notebook execution on error\n",
        "    mssparkutils.notebook.exit(f\"Failed to read stream data: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "stockPredict",
              "statement_id": 4,
              "statement_ids": [
                4
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "39",
              "normalized_state": "finished",
              "queued_time": "2025-05-05T22:05:02.622285Z",
              "session_start_time": null,
              "execution_start_time": "2025-05-05T22:05:02.6233696Z",
              "execution_finish_time": "2025-05-05T22:05:36.3668164Z",
              "parent_msg_id": "7d6fe69e-ca7c-4b23-b40c-3d2b66e60fbf"
            },
            "text/plain": "StatementMeta(stockPredict, 39, 4, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "# Step 3: Load Preprocessing Pipeline and Model\n",
        "from pyspark.ml import PipelineModel\n",
        "from pyspark.ml.regression import RandomForestRegressionModel\n",
        "\n",
        "# Define paths to model artifacts in ADLS Gen2\n",
        "pipeline_path = \"wasbs://team12blobcontainer@team12storage.blob.core.windows.net/Model/preprocess_pipeline\"\n",
        "model_path = \"wasbs://team12blobcontainer@team12storage.blob.core.windows.net/Model/stock_volume_rf_model\"\n",
        "\n",
        "# Load the trained pipeline and model\n",
        "preprocessing_pipeline = PipelineModel.load(pipeline_path)\n",
        "trained_model = RandomForestRegressionModel.load(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "stockPredict",
              "statement_id": 5,
              "statement_ids": [
                5
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "39",
              "normalized_state": "finished",
              "queued_time": "2025-05-05T22:06:04.0875948Z",
              "session_start_time": null,
              "execution_start_time": "2025-05-05T22:06:04.0889122Z",
              "execution_finish_time": "2025-05-05T22:06:34.2454478Z",
              "parent_msg_id": "37531bc4-3f36-43bc-bc90-83bdc7ee8fee"
            },
            "text/plain": "StatementMeta(stockPredict, 39, 5, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "db8d025b-44c4-4bd5-8649-14232a82a748",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": "SynapseWidget(Synapse.DataFrame, db8d025b-44c4-4bd5-8649-14232a82a748)"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# Step 4: Transform and Predict\n",
        "from pyspark.sql.functions import col, avg, current_timestamp\n",
        "# Manually cast boolean columns to integers\n",
        "streaming_df = streaming_df.withColumn(\"is_viral_int\", col(\"is_viral\").cast(\"int\")) \\\n",
        "    .withColumn(\"is_new_account_int\", col(\"is_new_account\").cast(\"int\")) \\\n",
        "    .withColumn(\"is_blue_verified_int\", col(\"is_blue_verified\").cast(\"int\")) \\\n",
        "    .withColumn(\"is_influencer_int\", col(\"is_influencer\").cast(\"int\"))\n",
        "\n",
        "# Apply pipeline\n",
        "transformed_df = preprocessing_pipeline.transform(streaming_df)\n",
        "\n",
        "# Predict\n",
        "predicted_df = trained_model.transform(transformed_df)\n",
        "\n",
        "# # Average prediction per day\n",
        "final_prediction_df = predicted_df.groupBy(\"tweet_created_at_date\") \\\n",
        "    .agg(avg(\"prediction\").alias(\"predicted_next_day_volume\"))\n",
        "\n",
        "display(final_prediction_df.limit(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "stockPredict",
              "statement_id": 6,
              "statement_ids": [
                6
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "39",
              "normalized_state": "finished",
              "queued_time": "2025-05-05T22:06:37.9123789Z",
              "session_start_time": null,
              "execution_start_time": "2025-05-05T22:06:37.913558Z",
              "execution_finish_time": "2025-05-05T22:06:48.5050231Z",
              "parent_msg_id": "cab9b2fb-bb9a-4944-8f14-ed9b40273382"
            },
            "text/plain": "StatementMeta(stockPredict, 39, 6, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "# Add date partition column\n",
        "final_prediction_df = final_prediction_df.withColumn(\"prediction_date\", col(\"tweet_created_at_date\"))\n",
        "\n",
        "prediction_date = final_prediction_df.select(\"prediction_date\").first()[0]\n",
        "\n",
        "output_path = f\"wasbs://team12blobcontainer@team12storage.blob.core.windows.net/Prediction/Streaming/{prediction_date}\"\n",
        "\n",
        "# use delta format to save\n",
        "(final_prediction_df\n",
        " .write\n",
        " .format(\"delta\")\n",
        " .mode(\"overwrite\") \n",
        " .save(output_path))"
      ]
    }
  ]
}